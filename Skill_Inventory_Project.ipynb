{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textract\n",
    "text = textract.process(\"Liam_Jackson_resume.docx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b\"Liam Jackson, Junior Data Engineer\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n     \\n\\n     \\n\\nLiam Jackson\\n\\nJunior Data Engineer\\n\\n\\n\\nLiam Jackson is a Junior Data Engineer at Gathi Analytics. \\n\\nAs a recent graduate with a Bachelor's Degree in Economics from Texas A&M University, Corpus Christi, and with a Data Science Certification from Codeup, Liam has extensive knowledge of data science tools and processes that allow him to take on large-scale analytical challenges. Liam has a good understanding of how data can help businesses realize their full potential, warn against natural disasters, and predict future outcomes. \\n\\nLiam strives to assist clients in data gathering, complex analysis, validation/reconciliation, and solving complex problems with data-driven solutions.\\n\\n\\n\\nPROFESSIONAL EXPERIENCE\\n\\nGathi Analytics\\n\\n2021 \\xe2\\x80\\x93 Present\\n\\nJunior Data Engineer\\n\\nAssists in data analysis, data reconciliation, requirements gathering, and developing solutions related to complex data and reporting issues\\n\\nDrives strategic process improvement engagements through current state assessment and business process automation efforts\\n\\nCreates project artifacts including presentations aligned to client deliverables and requirements\\n\\nAssists with project management tasks including project plan, issues and risk tracking and developing project process improvement initiatives\\n\\nData Science Projects\\n\\nFake News Capstone\\n\\nThe goal for this project was to identify specific phrases and words within fake news and real news articles. These labeled features could be used to accurately predict if a randomly sourced article was filled with misleading or incorrect information:\\n\\nUtilized Multiple NLP tools such as NLTK, bigrams, and trigrams to visualize the occurrences of fake news-related words\\n\\nBuilt a model that accurately predicted misleading information to warn the general public on what to look out for when reading news\\n\\nPredicting SPACEX Code\\n\\nThe goal of this project was to build a model that could predict which programming language a SpaceX-related repository used, given the text of the README file:\\n\\nUtilized Beautiful Soup to create a list of repo URLs to obtain the data\\n\\nIdentified diversification of programming languages used and the text from the README files\\n\\nPredicted 77% of SpaceX Python repositories using a logistic regression model\\n\\nPredicting Project Success\\n\\nThe goal was to identify features that could accurately measure the success of a Kickstarter project:\\n\\nData was acquired from Kaggle\\xe2\\x80\\x99s online database using various Python tools and classification models from the Sklearn libraries to determine which features correlated with the project\\xe2\\x80\\x99s success\\n\\nConcluded the features that defined a successful project were film and music-related that had a USD goal of $5000\\n\\nPredicting Churn\\n\\nThe goal of this project was to identify any features or a combination of features within the Telco dataset that could explain customer churn:\\n\\nUsed Pandas to acquire the CSV files and converted them into Pandas data frames for easier data manipulation\\n\\nAdopted Tableau to generate detailed visualizations of the findings\\n\\nDetermined that customers with fiberoptics and tenure of one to five months were three times more likely to churn compared to other product customers\\n\\n\\n\\n\\n\\nEDUCATION\\n\\nCodeup\\n\\n2020 \\xe2\\x80\\x93 2021\\n\\nCertificate of Completion \\n\\nFully immersive, project-based 22-week Data Science career accelerator that provided students with 670+ hours of expert instruction in applied data science\\n\\nTexas A&M University, Corpus Christi\\n\\n2017 - 2020\\n\\nB.B.A Economics\\n\\n\\n\\nSkills\\n\\nSQL\\n\\nPython\\n\\nPandas\\n\\nTableau\\n\\nExcel\\n\\nNumPy\\n\\nMatplotlib\\n\\nSpark\\n\\nSklearn\\n\\nSciPy\\n\\nSeaborn\\n\\nNLTK\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n2\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n2\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2 = textract.process(\"Stephen_Kane_Resume.docx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'Stephen Kane, Junior Data Engineer\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n     \\n\\n     \\n\\nStephen Kane\\n\\nJunior Data Engineer\\n\\n\\n\\n\\n\\nStephen Kane is a Junior Data Engineer at Gathi and a recent graduate from The University of Texas at San Antonio. \\n\\nAs a recent graduate in Business Administration in Cyber Security, Stephen brings his knowledge in cyber security while applying his data analyzing skills by utilizing data cleansing, data transformation and data manipulation techniques. Stephen has used some advanced data techniques and programing languages during his graduate program.\\n\\nStephen prides in his organized and detailed approach to data science and cyber security problems. He is a big thinker and a very creative problem solver who strives to assist clients in data analysis, data visualization, requirement gathering and data reconciliation programs.\\n\\n\\n\\n\\n\\nPROFESSIONAL EXPERIENCE\\n\\nGathi Analytics\\n\\n2021 \\xe2\\x80\\x93 Present\\n\\nJunior Data Engineer\\n\\nAssists in data analysis, data reconciliation, requirements gathering, and developing solutions related to complex data and reporting issues\\n\\nAssists in data science and cyber security areas\\n\\nCreates project artifacts including presentations aligned to client deliverables and requirements\\n\\nAssists with project management tasks including project plan, issues and risk tracking and developing project process improvement initiatives\\n\\nHEB\\n\\n2015 \\xe2\\x80\\x93 2021\\n\\nSeafood Specialist\\n\\nOversaw and coordinated daily meal production:\\n\\nAssisted in daily coordination of daily Meal Simple production resulting in a 16% sales increase over a six-month period\\n\\nSupervised and trained new hires with a focus on customer service\\n\\nHalf Price Books\\n\\n2014 \\xe2\\x80\\x93 2015\\n\\nBook Seller\\n\\nSales and Marketing Support activities:\\n\\nDesigned and implemented a plan to boost sales and marketing for the business section\\n\\nIncreased section sales by 14% over six-month period\\n\\nSupervised and trained new hires\\n\\nAcademic Projects\\n\\nOff the Rails: Capstone Project\\n\\nThe goal was to analyze over 8 years of the Department of Transportation\\xe2\\x80\\x99s dataset of rail accidents across United States:\\n\\nUtilized full data science pipeline to analyze the data and create a classification model\\n\\nInterpretated and reported findings \\n\\n\\n\\nNLP Web Scraping\\n\\nThe goal was to explore 400 GitHub repositories to predict the programming languages in the repository:\\n\\nUtilized GitHub\\xe2\\x80\\x99s API and Python\\xe2\\x80\\x99s BeautifulSoup to scrap over 400 GitHub repositories to extract the texts from README files\\n\\nAnalyzed the text using Natural Language Processing (NLP) techniques and developed multiple models to predict the programing languages in the repository\\n\\nExperimented with oversampling using SMOTE due to an extremely imbalanced dataset\\n\\nAchieved 60% accuracy based on the models developed\\n\\n\\n\\nNetwork Intrusion Detection\\n\\nThe goal was to identify anomalous activities using a Network Intrusion data set:\\n\\nUtilized Network intrusion data set from Kaggle to develop predictive models while isolating suspicious activities\\n\\nDeveloped several classification models using Scikit-learn for prediction\\n\\n \\n\\n\\n\\nEDUCATION\\n\\nCodeup\\n\\n2020 \\xe2\\x80\\x93 2021\\n\\nCertificate of Completion \\n\\n\\n\\nFully immersive, project-based 22-week Data Science career accelerator that provides students with 670+ hours of expert instruction in applied data science.\\n\\nThe University of Texas \\n\\n2016 \\xe2\\x80\\x93 2020\\n\\nBachelor\\xe2\\x80\\x99s Degree Business Administration in Cyber Security\\n\\n\\n\\nSkills & Languages\\n\\nSQL\\n\\nPython\\n\\nGit\\n\\nJupyter\\n\\nSklearn\\n\\nPandas\\n\\nMatplotlib\\n\\nTableau\\n\\nNumPy\\n\\nSeaborn\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n2\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n2'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text3 = textract.process(\"Micheal_Darkoh_resume.docx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'Michael Darkoh, Junior Data Engineer\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n     \\n\\n     \\n\\nMichael Darkoh\\n\\nJunior data Engineer\\n\\n\\n\\n\\n\\n\\n\\nMichael Darkoh is a Junior Data Engineer at Gathi as a recent graduate from The University of Texas. \\n\\nAs a recent graduate in Bachelor of Science in Actuarial Science, Minor in Mathematics and Statistics, Michael brings his knowledge and experience in analyzing data sets utilizing data cleansing, data transformation and data manipulation techniques. Given his background in statistics he is able to apply statistical analysis and other analytical approaches to assist in decision making.\\n\\nMichael strives to assist clients in data analysis, data visualization, developing business rules and statistical analysis while solving complex problems for his clients.\\n\\n\\n\\nPROFESSIONAL EXPERIENCE\\n\\nGathi Analytics\\n\\n2021 \\xe2\\x80\\x93 Present\\n\\nJunior Data Engineer\\n\\nAssists in data analysis, data reconciliation, requirements gathering, and developing solutions related to complex data and reporting issues\\n\\nAssists in data modeling and developing data cleansing business rules\\n\\nCreates project artifacts including presentations aligned to client deliverables and requirements\\n\\nAssists with project management tasks including project plan, issues and risk tracking and developing project process improvement initiatives\\n\\nLowe\\xe2\\x80\\x99s\\n\\n2017 \\xe2\\x80\\x93 2021\\n\\nCustomer Service III \\xe2\\x80\\x93 Sales Associate (Specialist)\\n\\nResponsible to analyzing and gathering daily sales orders data set and assisting in order management activities:\\n\\nPulled and cleaned order data with excel for deliveries worth more than $10,000 daily \\n\\nUsed Sterling Order Management System to place orders worth over $150,000 for customers \\n\\nMentored more than 20 new associates on work ethnics and safety protocols \\n\\nAcademic Projects\\n\\nAnalyzing Summer and Winter Olympic Games Dataset \\n\\nThe goal was to analyze Olympic games data set from year 1896 through 2014 to develop a world map that identifies the number of medals won by each country:\\n\\nUtilized R code to find the number of medals, won by each country, represented in a map\\n\\nInterpretated and reported findings in graphs for presentations using RStudio \\n\\nUsed R and RStudio software to clean datasets containing more than 16 rows\\n\\nPerformed statistical analyses with analytical approaches and used confidence interval and significant level to make statistical decisions\\n\\nTested data for errors and significant differences between samples and calculated medians quantities, percentiles and coefficients of correlations\\n\\nSales and Customer Dataset Analysis\\n\\nThe goal was to explore various data management aspects using sales and customer data set:\\n\\nDeveloped Entity \\xe2\\x80\\x93 relationship model to represent business rules of various problem set \\n\\nCreated ERD for database using Lucid Chart \\n\\nPerformed data cleaning, data manipulation, and analysis with MySQL to identify trends\\n\\nDebug code to recognize and fix errors\\n\\nVisual data using tableau to summarize findings to over 100 students \\n\\n\\n\\nEDUCATION\\n\\nThe University\\n\\nOf Texas at San Antonio\\n\\n2021 \\xe2\\x80\\x93 2023\\n\\n\\n\\nThe University of Texas at Dallas\\n\\n2018 \\xe2\\x80\\x93 2021\\n\\n\\n\\nMaster of Science Statistics and Data Science\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nBachelor of Science Actuarial Science; Minor in Mathematics and Statistics\\n\\n\\n\\nSkills & Languages\\n\\nSQL\\n\\nR\\n\\nMATLAB\\n\\nC++\\n\\nSnowflake basics\\n\\nRStudio\\n\\nLucid Chart\\n\\nTableau\\n\\nSAS\\n\\nWord\\n\\nExcel\\n\\nPowerPoint\\n\\nDBT Tool\\n\\n\\n\\nLanguages: English, Twi, Fanti and Hausa\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n2\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n2'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "email = re.search(rb'[\\w\\.-]+@[a-z0-9\\.-]+', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "email = re.search(rb'[\\w.+-]+@[\\w-]+\\.[\\w.-]+', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "email2 = re.search(rb'[\\w\\.-]+@[a-z0-9\\.-]+', text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(email2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "email3 = re.search(rb'[\\w\\.-]+@[a-z0-9\\.-]+', text3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(email3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_name = re.search(rb'([a-zA-Z]{3,30}\\s*)+', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(full_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_name2 = re.search(rb'([a-zA-Z]{3,30}\\s*)+', text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(full_name2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_name3 = re.search(rb'([a-zA-Z]{3,30}\\s*)+', text3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(full_name3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a data frame with these new values then export to excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "des = re.search(rb'(?<=Skills)[^.\\s]*', text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(des)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/Users/liamjackson/Desktop/Resumes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required module\n",
    "import os\n",
    "# assign directory\n",
    "directory = '/Users/liamjackson/Desktop/Resumes'\n",
    "\n",
    "# iterate over files in\n",
    "# that directory\n",
    "for filename in os.listdir(directory):\n",
    "    f = os.path.join(directory, filename)\n",
    "    # checking if it is a file\n",
    "    if os.path.isfile(f):\n",
    "        print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ['/Users/liamjackson/Desktop/Resumes/stephen_kane_DE_2021_verticle_resume.pdf',\n",
    "        '/Users/liamjackson/Desktop/Resumes/Liam_Jackson_Resume1E.pdf',\n",
    "        '/Users/liamjackson/Desktop/Resumes/Michael.pdf']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files:\n",
    "    res = textract.process(file)\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emails = re.findall(rb'[\\w\\.-]+@[a-z0-9\\.-]+', res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(emails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
